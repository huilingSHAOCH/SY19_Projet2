?cor
install.packages("ISLR")
library("ISLR")
college
data <- college
data <- data(college)
data <- data(College)
College
View(College)
O[1,] <- c(0,3,0)
?matrix
O <- matrix(data = NA, nrow = 6,ncol =3)
O[1,] <- c(0,3,0)
O[2,] <- c(2,0,0)
O[3.] <- c(0,1,3)
O[4,] <- c(0,1,2)
O[5,] <- c(-1,0,1)
O[6,] <- c(1,1,1)
O
O <- matrix(data = NA, nrow = 6,ncol =3)
O[1,] <- c(0,3,0)
O[2,] <- c(2,0,0)
O[3,] <- c(0,1,3)
O[4,] <- c(0,1,2)
O[5,] <- c(-1,0,1)
O[6,] <- c(1,1,1)
Otest <- c(0,0,0)
for (i in seq(1:6))
{
eudistance[i] <- sum((Otest - O[i,])^2)
}
O <- matrix(data = NA, nrow = 6,ncol =3)
eudistance <- matrix(data = NA,nrow = 6,ncol=1)
O[1,] <- c(0,3,0)
O[2,] <- c(2,0,0)
O[3,] <- c(0,1,3)
O[4,] <- c(0,1,2)
O[5,] <- c(-1,0,1)
O[6,] <- c(1,1,1)
Otest <- c(0,0,0)
for (i in seq(1:6))
{
eudistance[i] <- sum((Otest - O[i,])^2)
}
O <- matrix(data = NA, nrow = 6,ncol =3)
eudistance <- matrix(data = NA,nrow = 6,ncol=1)
O[1,] <- c(0,3,0)
O[2,] <- c(2,0,0)
O[3,] <- c(0,1,3)
O[4,] <- c(0,1,2)
O[5,] <- c(-1,0,1)
O[6,] <- c(1,1,1)
Otest <- c(0,0,0)
for (i in seq(1:6))
{
eudistance[i] <- sum((Otest - O[i,])^2)
}
eudistance
O <- matrix(data = NA, nrow = 6,ncol =3)
eudistance <- matrix(data = NA,nrow = 6,ncol=1)
O[1,] <- c(0,3,0)
O[2,] <- c(2,0,0)
O[3,] <- c(0,1,3)
O[4,] <- c(0,1,2)
O[5,] <- c(-1,0,1)
O[6,] <- c(1,1,1)
Otest <- c(0,0,0)
for (i in seq(1:6))
{
eudistance[i] <- sqrt(sum((Otest - O[i,])^2))
}
eudistance
expression<- read.table('expressions_train.txt')
I<-matrix(expression[1,],60,70)
I1 <- apply(I, 1, rev)
image(matrix(unlist(I1),ncol=70,byrow=TRUE),col=gray(0:255 / 255))
?colSums
library(klaR)
?NaiveBayes
setwd("~/Documents/SY19/projet2/SY19_Projet2")
classifieur_expressions <- function(dataset) {
# Chargement de l’environnement
load("env.Rdata")
#LDA renvoie le meilleur résultat
#Remove column full of 0 of training data
cleanDatatraining<-data_expressions[,colSums(abs(data_expressions[,1:ncol(data_expressions)-1])) !=0]
#FDA A CALCULER QUE POUR DONNEES TRAIN
library("MASS")
lda_data<- lda(y~.,data=cleanDatatraining)
U<-lda_data$scaling
X<-as.matrix(cleanDatatraining[,1:ncol(cleanDatatraining)-1])
Z<-X%*%U
Z<-as.data.frame(Z)
y<-cleanDatatraining$y
trainFDA<-cbind(Z,y)
#Remove column full of 0 of test data
cleanDatatest<-dataset[,colSums(abs(dataset[,1:ncol(dataset)-1])) !=0]
#Apply FDA on test data
X<-as.matrix(cleanDatatest[,1:ncol(cleanDatatest)-1])
Z<-X%*%U
Z<-as.data.frame(Z)
y<-cleanDatatest$y
testFDA<-cbind(Z,y)
#LDA
lda_data<- lda(y~.,data=trainFDA)
predictions<-predict(lda_data,newdata=testFDA)
predictions <- predictions$class
return(predictions)
}
classifieur_characters <- function(dataset) {
# Chargement de l’environnement
load("env.Rdata")
# Random Forest renvoie le meilleur résultat
library(randomForest)
#m = sqrt(p)
rdForest_data = randomForest(Y~., data=data_characters,mtry=4)
predictions = predict(rdForest_data, newdata=dataset, type = 'response')
return(predictions)
}
classifieur_parole <- function(dataset) {
# Chargement de l’environnement
load("env.Rdata")
# Bayésien naïf est donc le meilleur pour parole
#FDA A CALCULER QUE POUR DONNEES TRAIN
library("MASS")
lda_data<- lda(y~.,data=data_parole)
U<-lda_data$scaling
X<-as.matrix(data_parole[,1:ncol(data_parole)-1])
Z<-X%*%U
Z<-as.data.frame(Z)
y<-data_parole$y
trainFDA<-cbind(Z,y)
#Apply FDA on test data
X<-as.matrix(dataset[,1:ncol(dataset)-1])
Z<-X%*%U
Z<-as.data.frame(Z)
y<-dataset$y
testFDA<-cbind(Z,y)
library(klaR)
naivB_data<-NaiveBayes(y~.,data=trainFDA)
predictions<-predict(naivB_data,newdata=testFDA)
predictions <- predictions$class
return(predictions)
}
test_expressions<-read.table("expressions_train.txt")
test_expressions<-test_expressions[1:50,]
test_characters<-read.table("characters_train.txt")
test_characters<-test_characters[1:50,]
test_parole<-read.table("parole_train.txt")
test_parole<-test_parole[1:50,]
predexpression<-classifieur_expressions(test_expressions)
predcharacter<-classifieur_characters(test_characters)
predparole<-classifieur_parole(test_parole)
print('predexpression')
predexpression
print('predcharacter')
predcharacter
print('predparole')
predparole
