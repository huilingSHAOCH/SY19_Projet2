---
title: Apprentissage à partir de 3 jeux de données réels
output:
  html_document: default
  html_notebook: default
  pdf_document: default
---

Ce TP a pour but de proposer les meilleurs prédicteurs possibles pour traiter le problème de classification des données : 
- expression_train.txt, contenant les niveaux de gris d’une image associés a une expression du visage parmi joie, surprise, tristesse, dégoût, colère, peur;
- characters_train.txt, composé d’exemples issus de 26 classes correspondant aux 26 lettres de l’alphabet;
- parole_train.txt, dont chaque observation correspond à prononciation d’un phonème parmi "sh", "dcl", "iy", "aa" et "ao" 

Nous allons donc comparer sur ces jeu de données des qualités de prévision de plusieurs modèles.


```{r}
data_expressions<-read.table("expressions_train.txt")
data_characters<-read.table("characters_train.txt")
data_parole<-read.table("parole_train.txt")
```

#1 Données expression_train.txt

```{r}
#Generate train data and test data
data<-data_expressions
set.seed(2000)

#Generate train data and test data
N<-nrow(data)
nTrain=floor(3/4*N)
nTst=N-nTrain

trainIdx<-sample(1:N, nTrain)
train<-data[trainIdx,]
test<-data[-trainIdx,]
```

La suite du travail nécessite un échantillon d’apprentissage pour estimer les modèles et un échantillon de test pour comparer les erreurs de prévision.
Nous séparons donc le jeu de données en un ensemble de test comportant un quart des données et un ensemble d'apprentissage composé des données restantes. 

Nous remarquons un certain nombre de problèmes sur ce jeu de données. 

Tout d'abord, il existe des colonnes entièrement nulles qui génèrent des erreurs lors de l'aplication méthodes d'apprentissage. Il s'agit des prédicteurs correspondant aux coins au bas des images. Nous avons donc choisi de supprimer ces derniers puisqu'ils n'apportent aucune information. De plus, les images sont similaires (même cadrage du visage, ...) : les variables de ce jeu de données sont donc fortement corrélées. 

Il y a également un très grand nombre de prédicteurs comparé au nombre d'observations. Nous avons donc choisit d'effectuer une réduction de dimension par FDA.


```{r}
#Remove column full of 0
cleanData.train<-train[,colSums(abs(train[,1:ncol(train)-1])) !=0]
cleanData.test<-test[,colSums(abs(test[,1:ncol(test)-1])) !=0]

#FDA (with train data)
library("MASS")
lda_data<- lda(y~.,data=cleanData.train) 
U<-lda_data$scaling
X<-as.matrix(cleanData.train[,1:ncol(cleanData.train)-1])
Z<-X%*%U
Z<-as.data.frame(Z)
y<-cleanData.train$y
trainFDA<-cbind(Z,y)

#Apply FDA on test data
X<-as.matrix(cleanData.test[,1:ncol(cleanData.test)-1])
Z<-X%*%U
Z<-as.data.frame(Z)
y<-cleanData.test$y
testFDA<-cbind(Z,y)

#Plot
plot(trainFDA[,1], trainFDA[,2], col = trainFDA$y)

```

##1.1 K plus proches voisins

```{r}
library(class)
train.X <- trainFDA[,1:ncol(trainFDA)-1]
train.Y <- trainFDA[,ncol(trainFDA)]
test.X <- testFDA[,1:ncol(testFDA)-1]
test.Y <- testFDA[,ncol(testFDA)]

nbvoisin <- seq(1,10) 
error <- rep(1:10)
for (i in (1:10)){
  knn.pred=knn(train.X,test.X,train.Y,k=i)
  table(knn.pred,test.Y)
  error[i] <- 1 - mean(knn.pred == test.Y)
}
plot(nbvoisin,error,xlab="Nombre de voisin pris pour apprentissage",ylab="Erreur estimée")
```

Selon le graphique, on obtient l'erreur minimale de 0.18 quand k = 6. La méthode des K plus proches voisins a généralement de mauvais résultats en grande dimension puisque les voisins se retrouve en réalité très loin (fléau de la dimension). Le fait d'avoir effectué au préalable une réduction de dimension a permis d'obtenir de bons résultats, donc une bonne génératlisation et pas d'overfitting.

##1.2 Analyse Discriminante

Nous regarderons dans un premier temps les résultats des classifieurs obtenus par analyse discriminante linéaire et quadratique et du classifieur Bayésien naïf. Nous chercherons ensuite un modèle optimal en appliquant le principe de régularisation .

###1.2.1 LDA
```{r}
lda_data<- lda(y~.,data=trainFDA)
pred<-predict(lda_data,newdata=testFDA)
table<-table(testFDA$y,pred$class)
table
error<-1-sum(diag(table))/nTst
error
```

Nous obtenons un taux d'erreur de 0.14. Des frontières linéaires donnent donc un résultat satisfaisant.

###1.2.2 QDA
```{r}
qda_data<- qda(y~.,data=trainFDA)
pred<-predict(qda_data,newdata=testFDA)
table<-table(testFDA$y,pred$class)
table
error<-1-sum(diag(table))/nTst
error
```
Nous obtenons un taux d'erreur très élevé : 0.40. Ce classifieur est probablement trop flexible pour nos données, il est donc à exclure.

###1.2.3 Classifieur Bayésien naif

```{r}
library(klaR)

naivB_data<-NaiveBayes(y~.,data=trainFDA)
pred<-predict(naivB_data,newdata=testFDA)
table<-table(testFDA$y,pred$class)
table
error<-1-sum(diag(table))/nTst
error
```
Nous obtenons un taux d'erreur de 0.25. Ce dernier est donc moins bon que celui obtenu avec la LDA (0.14)

###1.2.4 Analyse discriminante régularisée
```{r}
rda_data <- rda(y~.,data=trainFDA, crossval = TRUE)
pred<-predict(rda_data,newdata=testFDA)
table<-table(testFDA$y,pred$class)
table
error<-1-sum(diag(table))/nTst
error
```

Le taux d'erreur est 0.18. Il est relativement proche de celui obtenu avec la LDA.

##1.3 Régression Logistique (GLM & GAM)

Ce problème de classification n'est pas une classification binaire. Donc on ne fera pas régression logistique car la méthode régression logistique réalisée par GLM et GAM (qui est une extension de GLM) est souvent moins performante que analyse discriminante (LDA,QDA...) pour les classifications avec plus de deux classes.

##1.4 Mixture Discriminant Analysis

```{r}
library(mclust)
ind_y = 6
MclustDa_data <- MclustDA(trainFDA[,1:ind_y-1],trainFDA[,ind_y])
#general covariance structure selected by BIC
summary(MclustDa_data, newdata = testFDA[,1:ind_y-1], newclass = testFDA[,ind_y])
```
Le summary nous montre le nombre de clusters (G) et le modèle Parsimonious (Model) dans chaque class en utilisant la méthode de modèle de sélection (critère BIC).

Malgré une erreur d'apprentissage nulle, l'erreur de test est extrêmement élevée (0.62). Ce modèle, comme la QDA, colle donc probablement beaucoup trop au données d'apprentissage (surapprentissage). Il faut donc privilégier pour ces données des classifieurs plus simples qui vont pouvoir extraire des tendances plus générales.

##1.5 Arbres

###1.5.1 Arbre de décision
```{r}
#install.packages("tree")
library(tree)

#Full tree
tree_data = tree(as.factor(y)~., trainFDA)
plot(tree_data)
text(tree_data, pretty = 0)

#Cross validation
size<-cv.tree(tree_data)$size
DEV<-rep(0, length(size))

for (i in (1:10)) 
{
  cv_data = cv.tree(tree_data)
  DEV<-DEV+cv_data$dev
}

DEV <- DEV/10

plot(cv_data$size, DEV, type = 'b')

#Pruning
prune_data = prune.tree(tree_data, best = 8)
plot(prune_data)
text(prune_data, pretty = 0)

#Test Error
y_pred = predict(prune_data, newdata = testFDA, type = 'class')
table<-table(y_pred, testFDA$y)
table
error<-1-sum(diag(table))/nTst
error
```

L'erreur obtenue est 0.44. Elle n'est pas acceptable. Nous allons tenter de l'améliorer en utilisant les méthodes de Bagging et de Random Forest. 

###1.5.2 Bagging
```{r}
#install.packages("randomForest")
library(randomForest)
#m =p = 5
bag_data = randomForest(y~., data=trainFDA, mtry=5)
ypred = predict(bag_data, newdata=testFDA, type = 'response')
table<-table(ypred, testFDA$y)
error<-1-sum(diag(table))/nTst
error
```
On obtient l'erreur 0.29.

###1.5.3 Random Forest
```{r}
#m = sqrt(p) = p/2 
rdForest_data = randomForest(y~., data=trainFDA,mtry=2)
ypred = predict(rdForest_data, newdata=testFDA, type = 'response')
table<-table(ypred, testFDA$y)
error<-1-sum(diag(table))/nTst
error
```

On obtient l'erreur 0.29. Ici on prend mtry = sqrt(p) = p/2 = 2.

Les erreurs obtenues par bagging et random forest sont plus faibles que celle obtenue initialement par l'arbre. Néanmoins, ces classifieurs n'égalent pas l'analyse discriminante linéaire.

##1.6 Support Vector Machine

```{r}
#install.packages("e1071")
library(e1071)

#Kernel = radial
tune.out = tune(svm, y~., data = trainFDA, kernel = "radial", range = list(cost=c(0.01, 0.1, 1, 10, 100), gamma = c(0.1, 1, 10)))
summary(tune.out)

svm_data<-svm(y~., data = trainFDA, kernel = "radial", gamma = 0.1, cost = 1)
ypred = predict(svm_data, newdata=testFDA)
table<-table(ypred, testFDA$y)
table
error<-1-sum(diag(table))/nTst
error
```

On obtient l'erreur 0.22.

Donc le classifieur obtenu par lda est le meilleur pour data_expressions.

#2 Données characters_train.txt
```{r}
#Generate train data and test data
data<-data_characters
set.seed(2000)

#Generate train data and test data
N<-nrow(data)
nTrain=floor(3/4*N)
nTst=N-nTrain

trainIdx<-sample(1:N, nTrain)
train<-data[trainIdx,]
test<-data[-trainIdx,]
```

##2.1 K plus proches voisins
```{r}
library(class)
idx_y = 1
train.X <- train[,(idx_y+1):ncol(train)]
train.Y <- train[,idx_y]
test.X <- test[,(idx_y+1):ncol(test)]
test.Y <- test[,idx_y]

nbvoisin <- seq(1,10) 
error <- rep(1:10)
for (i in (1:10)){
  knn.pred=knn(train.X,test.X,train.Y,k=i)
  table(knn.pred,test.Y)
  error[i] <- 1 - mean(knn.pred == test.Y)
}
plot(nbvoisin,error,xlab="Nombre de voisin pris pour apprentissage",ylab="Erreur estimée")
```


```{r}
lda_data<- lda(Y~.,data=train)
pred<-predict(lda_data,newdata=test)
table<-table(test$Y,pred$class)
table
error<-1-sum(diag(table))/nTst
error
```



#3 Données parole_train.txt
```{r}
#Generate train data and test data
data<-data_parole
set.seed(2000)

#Generate train data and test data
N<-nrow(data)
nTrain=floor(3/4*N)
nTst=N-nTrain

trainIdx<-sample(1:N, nTrain)
train<-data[trainIdx,]
test<-data[-trainIdx,]
```


```{r}
lda_data<- lda(y~.,data=train)
pred<-predict(lda_data,newdata=test)
table<-table(test$y,pred$class)
table
error<-1-sum(diag(table))/nTst
error
```

