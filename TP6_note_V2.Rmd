---
title: Apprentissage à partir de 3 jeux de données réels
output:
  html_document: default
  html_notebook: default
  pdf_document: default
---
Ce TP a pour but de proposer les meilleurs prédicteurs possibles pour traiter le problème de classification des données : 
- expression_train.txt contenant les niveaux de gris d’une image associés a une expression du visage parmi joie, surprise, tristesse, dégoût, colère, peur;
- characters_train.txt composé d’exemples issus de 26 classes correspondant aux 26 lettres de l’alphabet;
- parole_train.txt, dont chaque observation correspond à prononciation d’un phonème parmi "sh", "dcl", "iy", "aa" et "ao" 

Nous allons donc comparer sur ces jeu de données des qualités de prévision de plusieurs modèles.

```{r}
data_expressions<-read.table("expressions_train.txt")
data_characters<-read.table("characters_train.txt")
data_parole<-read.table("parole_train.txt")
```

#1 Données expression_train.txt
```{r}
#Generate train data and test data
data<-data_expressions
set.seed(2000)

#Generate train data and test data
N<-nrow(data)
nTrain=floor(3/4*N)
nTst=N-nTrain

trainIdx<-sample(1:N, nTrain)
train<-data[trainIdx,]
test<-data[-trainIdx,]
```

La suite du travail nécessite un échantillon d’apprentissage pour estimer les modèles et un échantillon de test pour comparer les erreurs de prévision.

Nous séparons donc le jeu de données en un ensemble de test comportant un quart des données et un ensemble d'apprentissage composé des données restantes. 

Nous remarquons un certain nombre de problèmes sur ce jeu de données. Donc il faut effectuer le data cleaning : 

Tout d'abord, il existe des colonnes entièrement nulles qui génèrent des erreurs lors de l'aplication méthodes d'apprentissage. Il s'agit des prédicteurs correspodant aux coins au bas des images. Nous avons donc choisi de supprimer ces derniers. 

De plus, les images sont similaires (même cadrage du visage) : les variables de ce jeu de données sont donc fortement corrélées. Il y a également un très grand nombre de prédicteurs comparé au nombre d'observations. Nous allons donc effectuer une réduction de dimension par FDA.


```{r}
#Remove column full of 0
cleanData.train<-train[,colSums(abs(train[,1:ncol(train)-1])) !=0]
cleanData.test<-test[,colSums(abs(test[,1:ncol(test)-1])) !=0]

#FDA (with train data)
library("MASS")
lda_data<- lda(y~.,data=cleanData.train) 
U<-lda_data$scaling
X<-as.matrix(cleanData.train[,1:ncol(cleanData.train)-1])
Z<-X%*%U
Z<-as.data.frame(Z)
y<-cleanData.train$y
trainFDA<-cbind(Z,y)

#Apply FDA on test data
X<-as.matrix(cleanData.test[,1:ncol(cleanData.test)-1])
Z<-X%*%U
Z<-as.data.frame(Z)
y<-cleanData.test$y
testFDA<-cbind(Z,y)

#Plot
plot(trainFDA[,1], trainFDA[,2], col = trainFDA$y)

```
##1.1 K plus proches voisins
```{r}
library(class)
train.X <- trainFDA[,1:ncol(trainFDA)-1]
train.Y <- trainFDA[,ncol(trainFDA)]
test.X <- testFDA[,1:ncol(testFDA)-1]
test.Y <- testFDA[,ncol(testFDA)]

nbvoisin <- seq(1,10) 
error <- rep(1:10)
for (i in (1:10)){
  knn.pred=knn(train.X,test.X,train.Y,k=i)
  table(knn.pred,test.Y)
  error[i] <- 1 - mean(knn.pred == test.Y)
}
plot(nbvoisin,error,xlab="Nombre de voisin pris pour apprentissage",ylab="Erreur estimée")
```

Selon le graph, on obtient l'erreur minimale 0.1851852 quand k = 6. En fait la méthode k plus proches voisins est une méthode non paramétrique et la donnée expressions a un grand nombre de prédicteurs mais peu de observations. Donc pour éviter le phénomène overfitting, il faut prendre une valeur relativement grande pour k (ici 6).

##1.2 Analyse Discriminante

###1.2.1 LDA
```{r}
lda_data<- lda(y~.,data=trainFDA)
pred<-predict(lda_data,newdata=testFDA)
table<-table(testFDA$y,pred$class)
table
error<-1-sum(diag(table))/nTst
error
```

On obtient l'erreur 0.1481481.

###1.2.2 QDA
```{r}
qda_data<- qda(y~.,data=trainFDA)
pred<-predict(qda_data,newdata=testFDA)
table<-table(testFDA$y,pred$class)
table
error<-1-sum(diag(table))/nTst
error
```
On obtient l'erreur 0.4074074. 

###1.2.3 Classifieur Bayésien naif

```{r}
library(klaR)

naivB_data<-NaiveBayes(y~.,data=trainFDA)
pred<-predict(naivB_data,newdata=testFDA)
table<-table(testFDA$y,pred$class)
table
error<-1-sum(diag(table))/nTst
error
```
On obtient l'erreur 0.07407407 qui est la même que l'erreur précédente. En fait LDA est une amélioration de Bayésien naif méthode avec plus de flexibilité. Si l'erreur de BN est la même que l'erreur de LDA, on peut déduire que les matrices de covariances de chaque class sont similaires et diagonales.

###1.2.4 Analyse discriminante régularisée
```{r}
rda_data <- rda(y~.,data=trainFDA, crossval = TRUE)
pred<-predict(rda_data,newdata=testFDA)
table<-table(testFDA$y,pred$class)
table
error<-1-sum(diag(table))/nTst
error
```

L'erreur est toujours 0.07407407. L'erreur de régularisation entre LDA et QDA reste la même parce que l'erreurs de QDA et LDA sont les mêmes.

##1.3 Régression Logistique (GLM & GAM)

Ce problème de classification n'est pas une classification binaire. Donc on ne fera pas régression logistique car la méthode régression logistique réalisée par GLM et GAM (qui est une extension de GLM) est souvent moins performante que analyse discriminante (LDA,QDA...) pour les classifications avec plus de deux classes.

##1.4 Mixture Discriminant Analysis

```{r}
library(mclust)
ind_y = 6
MclustDa_data <- MclustDA(trainFDA[,1:ind_y-1],trainFDA[,ind_y])
#general covariance structure selected by BIC
summary(MclustDa_data, newdata = testFDA[,1:ind_y-1], newclass = testFDA[,ind_y])
```
Le summary nous montre le nombre de clusters (G) et le modèle Parsimonious (Model) dans chaque class en utilisant la méthode de modèle de sélection (critère BIC).

On obtient l'erreur 0.2222222 qui est relativemen grande par rapport aux erreurs précédentes. Ici on constate que le MSE training est 0 qui est inférieur à l'erreur de Bayes (erreur irréductible pour MSE test). Donc on peut déduire que l'erreur est grande à cause de overfitting.

##1.5 Arbres

###1.5.1 Arbre de décision
```{r}
#install.packages("tree")
library(tree)

#Full tree
tree_data = tree(as.factor(y)~., trainFDA)
plot(tree_data)
text(tree_data, pretty = 0)

#Cross validation
size<-cv.tree(tree_data)$size
DEV<-rep(0, length(size))

for (i in (1:10)) 
{
  cv_data = cv.tree(tree_data)
  DEV<-DEV+cv_data$dev
}

DEV <- DEV/10

plot(cv_data$size, DEV, type = 'b')

#Pruning
prune_data = prune.tree(tree_data, best = 8)
plot(prune_data)
text(prune_data, pretty = 0)

#Test Error
y_pred = predict(prune_data, newdata = testFDA, type = 'class')
table<-table(y_pred, testFDA$y)
table
error<-1-sum(diag(table))/nTst
error
```

On obtient l'erreur 0.1481481.

###1.5.2 Bagging
```{r}
#install.packages("randomForest")
library(randomForest)
#m =p = 5
bag_data = randomForest(y~., data=trainFDA, mtry=5)
ypred = predict(bag_data, newdata=testFDA, type = 'response')
table<-table(ypred, testFDA$y)
error<-1-sum(diag(table))/nTst
error
```
On obtient l'erreur 0.1111111.

###1.5.3 Random Forest
```{r}
#m = sqrt(p) = p/2 
rdForest_data = randomForest(y~., data=trainFDA,mtry=2)
ypred = predict(rdForest_data, newdata=testFDA, type = 'response')
table<-table(ypred, testFDA$y)
error<-1-sum(diag(table))/nTst
error
```

On obtient l'erreur 0.07407407. Ici on prend mtry = sqrt(p) = p/2 = 2.

##1.6 Support Vector Machine

```{r}
#install.packages("e1071")
library(e1071)

#Kernel = radial
tune.out = tune(svm, y~., data = trainFDA, kernel = "radial", range = list(cost=c(0.01, 0.1, 1, 10, 100), gamma = c(0.1, 1, 10)))
summary(tune.out)

svm_data<-svm(y~., data = trainFDA, kernel = "radial", gamma = 0.1, cost = 1)
ypred = predict(svm_data, newdata=testFDA)
table<-table(ypred, testFDA$y)
table
error<-1-sum(diag(table))/nTst
error
```

On obtient l'erreur 0.07407407.

Donc le clssifier K plus proches voisins est le meilleur pour data_expressions.


